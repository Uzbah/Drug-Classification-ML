# -*- coding: utf-8 -*-
"""DrugClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfFXql08w8fSmGtaCkFYrMEUsKA0taIr

1. **Importing necessary ibrary and uploading Drug Dataset for classification**
"""

import pandas as pd
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/AICourse/drug_classification - drug_classification.csv")

"""**Step 2: Dataset Exploration and Preprocessing**

First we start with EDA to check data is clean
"""

#check shape of data
df.shape

df.describe().T

df.info()

df.dtypes

df.isnull().sum()

df.duplicated().sum()

"""Splitting data"""

# Split features and target variable
X = df.drop("Drug_Type", axis=1)
y = df["Drug_Type"]

# Convert categorical variables to numerical using one-hot encoding
X = pd.get_dummies(X, columns=["Sex", "BP", "Cholesterol"], drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Step 3: Apply Classification Algorithms**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))
print("KNN Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Logistic Regression Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

from sklearn.metrics import classification_report

print("Tuned KNN Report:\n", classification_report(y_test, y_pred_knn))
print("Tuned RF Report:\n", classification_report(y_test, y_pred_rf))
print("Tuned LR Report:\n", classification_report(y_test, y_pred_lr))

"""**Pre-Tuning Results**
1. KNN Accuracy: 68%
2. Random Forest Accuracy: 100%
3. Linear Regression Accuracy: 97%

**Step 4: Hyperparameter Tuning**

KNN
"""

from sklearn.model_selection import GridSearchCV

param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_knn.fit(X_train, y_train)
print("Best KNN Parameters:", grid_knn.best_params_)
y_pred_knn_tuned = grid_knn.best_estimator_.predict(X_test)

"""Random Forest"""

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5)
grid_rf.fit(X_train, y_train)
print("Best RF Parameters:", grid_rf.best_params_)
y_pred_rf_tuned = grid_rf.best_estimator_.predict(X_test)

"""Linear Regression"""

param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}
grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5)
grid_lr.fit(X_train, y_train)
print("Best LR Parameters:", grid_lr.best_params_)
y_pred_lr_tuned = grid_lr.best_estimator_.predict(X_test)

"""**Step 5: Evaluate Tuned**"""

from sklearn.metrics import classification_report

print("Tuned KNN Report:\n", classification_report(y_test, y_pred_knn_tuned))
print("Tuned RF Report:\n", classification_report(y_test, y_pred_rf_tuned))
print("Tuned LR Report:\n", classification_report(y_test, y_pred_lr_tuned))

"""1. KNN Accuracy: 60%
2. Random Forest Accuracy: 100%
3. Linear Regreession Accuracy: 100%

**Observations**
1. **Best Performing Models:**
Random Forest and Logistic Regression achieved perfect accuracy, demonstrating their suitability for this dataset.
2. **KNN Performance:**
KNN struggled with imbalanced data and could not predict all classes effectively, despite hyperparameter tuning.
3. **Class Imbalance Impact:**
Classes like drugC and drugB had very few samples, making it harder for KNN to generalize.
"""